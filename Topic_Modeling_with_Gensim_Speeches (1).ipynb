{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-qgk3Ck6qHG"
      },
      "source": [
        "# Topic Modeling with Gensim (Python)\n",
        "\n",
        "1. Set up Pyspark in Colab\n",
        "2. Install nltk, Download nltk stopwords\n",
        "3. Stemming and Lemmatization\n",
        "    * Install Gensim\n",
        "    * Install spaCy\n",
        "    * Install other dependencies\n",
        "    * Download and import speeches\n",
        "4. LDA analysis\n",
        "    * Data Cleaning\n",
        "    * Bigrams and Trigrams\n",
        "    * Topic Modeling Inputs\n",
        "    * Optimal Number of Topics\n",
        "    * Viewing LDA topics\n",
        "5. LDA Evaluation\n",
        "    * Model Perplexity and Coherence\n",
        "    * Install pyLDAvis\n",
        "    * Install LDA Mallet\n",
        "6. Interpreting LDA Results\n",
        "    * Dominant topic in each sentence\n",
        "    * Most representative document for each topic\n",
        "    * Topic distribution across documents\n",
        "7. Conclusions\n",
        "    * Strengths of Topic Modeling\n",
        "    * Words of caution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq8U3BtmhtRx"
      },
      "source": [
        "\n",
        "## Set up Pyspark in Colab\n",
        "\n",
        "To run spark in Colab, we need to first install all the dependencies in Colab environment i.e. Apache Spark 2.3.2 with hadoop 2.7, Java 8 and Findspark to locate the spark in the system.\n",
        "Follow the steps to install the dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lh5NCoc8fsSO"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kpXBmifOrYgd"
      },
      "outputs": [],
      "source": [
        "# Can take as much as 6 minutes!\n",
        "\n",
        "!wget -qN https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7kaCVU3At7XT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"spark-3.2.1-bin-hadoop3.2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "N5w85-Vaq3ow"
      },
      "outputs": [],
      "source": [
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nEMK4HfP0ag"
      },
      "source": [
        "## 2. Install nltk, download nltk stopwords\n",
        "\n",
        "Definition: high-frequency words like *the, to, and, also* that we sometimes want to filter out of a document before further processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5T9ylYgLEEv",
        "outputId": "53a8c2c1-5968-438d-f6d5-df720288aca5"
      },
      "source": [
        "# Run in python console\n",
        "import nltk; nltk.download('stopwords')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LU8bySG4JMEy"
      },
      "source": [
        "Let's import the stopwords and make them available in `stop_words`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxK_dgu1JMEz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "192991dc-34ce-45d8-eb8f-027f454b9f1a"
      },
      "source": [
        "# NLTK Stop words\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "# Additional words we would like to ignore, for example, for a corpus of emails\n",
        "# stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
        "print(len(stop_words), stop_words)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "179 ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sn2QYXGgKl8k"
      },
      "source": [
        "## 3. Stemming and lemmatization\n",
        "\n",
        "For grammatical reasons, documents are going to use different forms of a word, such as organize, organizes, and organizing. Additionally, there are families of derivationally related words with similar meanings, such as democracy, democratic, and democratization. In many situations, it seems as if it would be useful for a search for one of these words to return documents that contain another word in the set.\n",
        "\n",
        "The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.\n",
        "\n",
        "For grammatical reasons, documents are going to use different forms of a word, such as organize, organizes, and organizing. Additionally, there are families of derivationally related words with similar meanings, such as democracy, democratic, and democratization. In many situations, it seems as if it would be useful for a search for one of these words to return documents that contain another word in the set.\n",
        "\n",
        "The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. For instance:\n",
        "\n",
        "**am, are, is $\\Rightarrow$ be**\n",
        "\n",
        "**car, cars, car's, cars' $\\Rightarrow$ car**\n",
        "\n",
        "The result of this mapping of text will be something like:\n",
        "\n",
        "**the boy's cars are different colors $\\Rightarrow$ the boy car be differ color**\n",
        "\n",
        "* *Stemming* usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. For example, **saw** $\\Rightarrow$ **s**.\n",
        "\n",
        "* *Lemmatization* usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma . For example, **saw**  ⇒ **see** or **saw**  ⇒ **saw** depending on whether the use of the token was as a verb or a noun.\n",
        "\n",
        "Stemming is fast and crude whereas lemmatization commonly only collapses the different inflectional forms of a lemma.\n",
        "\n",
        "Stemming or lemmatization is often done by an additional plug-in component to the indexing process, and a number of such components exist, both commercial and open-source.\n",
        "\n",
        "Source: [Stanford NLP](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHO7QOUsKORs"
      },
      "source": [
        "# installing other dependencies\n",
        "import pandas as pd\n",
        "# pd.set_option('display.max_rows', None)\n",
        "# pd.set_option('display.max_colwidth', None)\n",
        "import numpy as np\n",
        "\n",
        "import re\n",
        "import string\n",
        "import pprint\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gjPwI-UJME0"
      },
      "source": [
        "## 4. Import data\n",
        "\n",
        "Downloads and prepares a dictionary with\n",
        "\n",
        "* Keys = speech_name, e.g., `1789-Washington.txt` and\n",
        "* Value = List of list of sentences, e.g., [<br/>['Fellow', '-', 'Citizens', 'of', 'the', 'Senate', 'and', 'of', 'the', 'House', 'of', 'Representatives', ':'], <br/>['Among', 'the', 'vicissitudes', 'incident', 'to', 'life', 'no', 'event', 'could', 'have', 'filled', 'me', 'with', 'greater', 'anxieties', 'than', 'that', 'of', 'which', 'the', 'notification', 'was', 'transmitted', 'by', 'your', 'order', ',', 'and', 'received', 'on', 'the', '14th', 'day', 'of', 'the', 'present', 'month', '.'], <br/>...]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NHPCfpApgJb",
        "outputId": "21192db4-2769-41bb-9eda-c9e3d8f42e1a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mount user's Google Drive to Google Colab.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yl_5DmVJCb_w",
        "outputId": "abd0c9bb-9c95-49f4-9135-9e119f5f1002"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!ls\n",
        "import pandas as pd\n",
        "reviews_df = pd.read_csv('/content/drive/MyDrive/data/reviews.csv')  # Update the path\n",
        "\n",
        "# Extract relevant text column\n",
        "reviews_text = reviews_df['Review Text']\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCXy9DHyCdAP",
        "outputId": "d3eccdcc-3db4-4d8d-bd08-0a9c8b49ef8c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data  spark-3.2.1-bin-hadoop3.2  spark-3.2.1-bin-hadoop3.2.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0uCG78TJMEv"
      },
      "source": [
        "### 3. Install Gensim and spaCy model (for lemmatization)\n",
        "Lemmatization is nothing but converting a word to its root word. For example: the lemma of the word ‘machines’ is ‘machine’. Likewise, ‘walking’ –> ‘walk’, ‘mice’ –> ‘mouse’ and so on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCV4wCGt-pZz",
        "outputId": "439c4679-4466-4c4b-bc9a-2159b87b6c0b"
      },
      "source": [
        "!pip install gensim\n",
        "!python3 -m spacy download en_core_web_sm"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n",
            "2023-11-21 22:49:29.676736: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-21 22:49:29.676824: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-21 22:49:29.676875: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-21 22:49:29.693049: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-21 22:49:31.192341: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryoEpP4gMqI6"
      },
      "source": [
        "# Gensim\n",
        "\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# spacy for lemmatization\n",
        "import spacy\n",
        "spacy.load('en_core_web_sm')\n",
        "\n",
        "# Enable logging for gensim - optional\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMWsrdMYJME6"
      },
      "source": [
        "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'> 8. Tokenize words and Clean-up text </h2>\n",
        "</div>\n",
        "\n",
        "The sentences look better now, but you want to tokenize each sentence into a list of words, removing punctuations and unneccessary characters altogether.\n",
        "\n",
        "Gensim's `simple_preprocess()` is great for this. Additionally I have set `deacc=True` to remove the punctuations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEawaeOJJME6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f968f270-be71-4cb7-e712-b44207c378c2"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence).encode('utf-8'), deacc=True))  # deacc=True removes punctuations\n",
        "\n",
        "data_words = list(sent_to_words(reviews_text))\n",
        "data_words = [dw for dw in data_words if len(dw)>0]\n",
        "print(data_words[3401:3406])\n",
        "\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['love', 'the', 'swing', 'style', 'boxy', 'fit', 'versatile', 'works', 'well', 'over', 'jeans', 'or', 'any', 'pants', 'and', 'top', 'like', 'the', 'coated', 'linen', 'or', 'cotton', 'look', 'serves', 'much', 'the', 'same', 'purpose', 'as', 'any', 'light', 'weight', 'or', 'denim', 'jacket', 'good', 'to', 'wear', 'over', 'top', 'to', 'complete', 'an', 'outfit', 'or', 'as', 'an', 'extra', 'thin', 'layer', 'but', 'not', 'substantial', 'outdoor', 'jacket', 'really', 'quite', 'cute', 'fairly', 'tts', 'but', 'if', 'between', 'sizes', 'size', 'down'], ['the', 'top', 'was', 'very', 'small', 'am', 'usually', 'size', 'small', 'but', 'this', 'was', 'tight', 'around', 'the', 'top', 'arm', 'holes', 'and', 'it', 'was', 'short', 'also', 'didn', 'notice', 'if', 'it', 'said', 'it', 'was', 'sheer', 'see', 'through', 'in', 'the', 'description', 'but', 'it', 'definitely', 'requires', 'another', 'shirt', 'underneath'], ['ve', 'had', 'this', 'jacket', 'for', 'few', 'weeks', 'and', 'it', 'has', 'been', 'terrific', 'transition', 'piece', 'for', 'the', 'spring', 'this', 'is', 'lightweight', 'layering', 'piece', 'without', 'any', 'lining', 'it', 'works', 'well', 'as', 'topper', 'for', 'cooler', 'morning', 'temps', 'and', 'you', 'can', 'still', 'wear', 'it', 'later', 'in', 'the', 'day', 'without', 'perspiring', 'through', 'it', 'bought', 'it', 'in', 'the', 'olive', 'color', 'which', 'works', 'with', 'just', 'about', 'everything', 'and', 'the', 'xs', 'was', 'tts', 'for', 'reference', 'lbs', 'the', 'fit', 'is', 'slightly', 'boxy', 'with', 'nice', 'swing', 'and', 'drape', 'in', 'the', 'back', 'for', 'those', 'concerned', 'about', 'the', 'back'], ['am', 'pounds', 'with', 'ddd', 'bust', 'typically', 'purchase', 'small', 'or', 'to', 'in', 'blouses', 'this', 'blouse', 'was', 'small', 'across', 'the', 'chest', 'and', 'therefore', 'purchased', 'size', 'it', 'is', 'nice', 'fit', 'however', 'the', 'button', 'placement', 'on', 'the', 'blouse', 'does', 'not', 'work', 'well', 'for', 'someone', 'who', 'is', 'chesty', 'one', 'button', 'is', 'placed', 'slightly', 'above', 'bust', 'line', 'and', 'the', 'other', 'slightly', 'below', 'this', 'causes', 'peek', 'boo', 'space', 'between', 'buttons', 'therefore', 'wear', 'bright', 'white', 'tank', 'top', 'beneath', 'the', 'blouse', 'and', 'wear', 'the', 'blouse', 'unbuttoned', 'to'], ['loved', 'the', 'print', 'on', 'both', 'but', 'first', 'bought', 'the', 'blue', 'to', 'check', 'out', 'the', 'fit', 'and', 'feel', 'went', 'ahead', 'and', 'ordered', 'the', 'cat', 'one', 'today', 'the', 'fit', 'is', 'relaxed', 'and', 'slightly', 'oversized', 'so', 'if', 'you', 'want', 'slimmer', 'or', 'less', 'boxy', 'fit', 'size', 'down', 'for', 'myself', 'was', 'perfect', 'since', 'built', 'larger', 'and', 'broader', 'on', 'top', 'the', 'top', 'is', 'bit', 'long', 'covers', 'my', 'hips', 'and', 'some', 'of', 'my', 'bottom', 'it', 'mixed', 'fabric', 'design', 'so', 'the', 'linen', 'part', 'may', 'feel', 'scratchy', 'to', 'sensitive', 'types']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSg33gKvJME6"
      },
      "source": [
        "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'> 9. Create Bigram and Trigram Models</h2>\n",
        "</div>\n",
        "\n",
        "Bigrams are two words frequently occurring together in the document. Trigrams are 3 words that occur frequently.\n",
        "\n",
        "Some examples in our example are: 'soviet union', etc.\n",
        "\n",
        "Gensim's `Phrases` model can build and implement the bigrams, trigrams, quadgrams and more. The two important arguments to `Phrases` are `min_count` and `threshold`. The higher the values of these param, the harder it is for words to be combined to bigrams."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrwK3tkBJME7",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34b3c484-d7e2-4046-ae05-c664fb1254a8"
      },
      "source": [
        "# Build the bigram and trigram models\n",
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
        "\n",
        "# Faster way to get a sentence clubbed as a trigram/bigram\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
        "\n",
        "# See trigram example\n",
        "print(trigram_mod[bigram_mod[data_words[0]]])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['absolutely', 'wonderful', 'silky', 'and', 'sexy', 'and', 'comfortable']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSKmvrrMJME7"
      },
      "source": [
        "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'> 10. Remove Stopwords, Make Bigrams and Lemmatize</h2>\n",
        "</div>\n",
        "\n",
        "\n",
        "The bigrams model is ready. Let's define the functions to remove the stopwords, make bigrams and lemmatization and call them sequentially."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIAmYQLkJME7"
      },
      "source": [
        "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "\n",
        "def make_bigrams(texts):\n",
        "    return [bigram_mod[doc] for doc in texts]\n",
        "\n",
        "def make_trigrams(texts):\n",
        "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent))\n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts_out"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vD4jR4UBygH"
      },
      "source": [
        "Let’s call the functions in order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGC-N8_EJME7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1e045df-59df-46ac-e6a8-c4a372c891a3"
      },
      "source": [
        "# Remove Stop Words\n",
        "data_words_nostops = remove_stopwords(data_words)\n",
        "print(data_words_nostops)\n",
        "\n",
        "# Form Bigrams\n",
        "data_words_bigrams = make_bigrams(data_words_nostops)\n",
        "print(data_words_bigrams)\n",
        "\n",
        "# In the end, we didn't create trigrams. Should have taken the extra time.\n",
        "\n",
        "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
        "# python3 -m spacy download en\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "\n",
        "# Do lemmatization keeping only noun, adj, vb, adv\n",
        "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "\n",
        "print(data_lemmatized[:1])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['absolutely', 'wonderful', 'silky', 'sexy', 'comfortable']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LSm3Lef6qHY"
      },
      "source": [
        "# pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7uMrXUpJME8"
      },
      "source": [
        "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'> 11. Create the Dictionary and Corpus needed for Topic Modeling</h2>\n",
        "</div>\n",
        "\n",
        "The two main inputs to the LDA topic model are the dictionary(`id2word`) and the corpus. Let's create them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqlOgG27JME8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f66497b3-74d0-4a75-cf92-7221cef28827"
      },
      "source": [
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(data_lemmatized)\n",
        "\n",
        "# Create Corpus\n",
        "texts = data_lemmatized\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "# View\n",
        "# Human readable format of corpus (term-frequency)\n",
        "# There is nothing magical about 2500:2510, I just wanted to examine a random location\n",
        "print ([[(id2word[id], freq) for id, freq in cp] for cp in corpus[2500:2512]])"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[('dress', 3), ('fit', 1), ('really', 1), ('somewhat', 1), ('work', 2), ('return', 1), ('look', 1), ('big', 2), ('nice', 1), ('material', 1), ('right', 1), ('chest', 1), ('large', 1), ('figure', 1), ('hope', 1), ('shape', 1), ('way', 1), ('think', 1), ('outfit', 1), ('cheste', 1), ('especially', 1), ('give', 1), ('unfortunately', 1), ('band', 1), ('consider', 1), ('staple', 1), ('wayyy', 1)], [('comfortable', 1), ('dress', 1), ('find', 1), ('fit', 2), ('size', 1), ('small', 1), ('time', 1), ('wear', 2), ('well', 2), ('long', 1), ('light', 1), ('cup', 1), ('fabric', 1), ('feel', 1), ('large', 1), ('open', 1), ('breast', 1), ('quite', 1), ('tend', 1), ('girl', 2), ('complaint', 1), ('refreshing', 1), ('suggest', 1), ('wind', 1), ('delightful', 1), ('clasp', 1), ('fly', 1), ('blow', 1), ('numerous', 1)], [('wonderful', 1), ('dress', 3), ('store', 1), ('nicely', 1), ('size', 1), ('great', 1), ('wear', 1), ('legging', 1), ('color', 1), ('style', 1), ('person', 1), ('nice', 1), ('drape', 1), ('picture', 1), ('short', 2), ('many', 1), ('jacket', 1), ('online', 1), ('boot', 2), ('walk', 1), ('actually', 1), ('however', 1), ('draw', 1), ('texture', 1), ('display', 1), ('cropped', 1), ('earthy', 1)], [('comfortable', 1), ('love', 1), ('fit', 1), ('size', 1), ('small', 1), ('usual', 1), ('work', 1), ('get', 3), ('front', 1), ('lot', 1), ('try', 1), ('big', 1), ('sleeve', 1), ('fabric', 1), ('back', 1), ('enough', 1), ('problem', 1), ('shape', 1), ('coat', 1), ('different', 1), ('sadly', 1), ('picture', 1), ('wait', 1), ('shoulder', 3), ('okay', 1), ('extra', 1), ('concern', 1), ('real_life', 1), ('believe', 1), ('maybe', 1), ('consider', 1), ('honestly', 1), ('hung', 1), ('deliver', 1), ('possibly', 1)], [('dress', 5), ('little', 1), ('fit', 1), ('really', 2), ('somewhat', 1), ('fun', 1), ('front', 1), ('perfect', 1), ('busty', 1), ('fabric', 1), ('make', 1), ('material', 1), ('flattering', 1), ('cute', 1), ('move', 2), ('short', 1), ('cut', 1), ('lbs', 1), ('walk', 1), ('asymmetrical', 1), ('detail', 1), ('seam', 1), ('lightweight', 1), ('pocket', 1), ('bring', 1), ('hem', 1), ('slightly', 1), ('tent', 1), ('flowy', 1), ('empire', 1), ('sheath', 1), ('gauzy', 1), ('ch', 1), ('positioned', 1)], [('love', 1), ('wear', 1), ('pair', 1), ('well', 1), ('skirt', 1), ('look', 1), ('see', 1), ('light', 1), ('nice', 1), ('sure', 1), ('even', 1), ('feel', 1), ('material', 1), ('fall', 1), ('blouse', 1), ('clean', 1), ('describe', 1), ('sweater', 1), ('week', 1), ('blue', 2), ('thin', 1), ('online', 1), ('boot', 1), ('winter', 1), ('grey', 1), ('black', 1), ('pass', 1), ('ruffle', 1), ('pattern', 1), ('almost', 1), ('easy', 1), ('strong', 1), ('silk', 1), ('next', 1), ('gal', 1), ('pear_shape', 1), ('slimme', 1), ('polyester', 1), ('motif', 1), ('aline', 1), ('background', 1), ('strappy_sandal', 1)], [('comfortable', 1), ('dress', 1), ('love', 1), ('true', 1), ('size', 1), ('wear', 2), ('nice', 1), ('run', 1), ('fabric', 1), ('simple', 1), ('season', 1), ('black', 1), ('amp', 1)], [('dress', 2), ('order', 1), ('really', 1), ('size', 1), ('wear', 1), ('large', 1), ('model', 1), ('guess', 1), ('extremely', 1), ('weigh', 1), ('like', 1), ('shame', 1)], [('comfortable', 1), ('dress', 3), ('size', 1), ('want', 1), ('get', 1), ('wear', 2), ('well', 1), ('long', 1), ('style', 1), ('usually', 1), ('decide', 1), ('look', 2), ('try', 1), ('turn', 1), ('bit', 1), ('loose', 2), ('material', 1), ('tall', 1), ('model', 1), ('lb', 1), ('high', 1), ('picture', 2), ('bright', 1), ('give', 1), ('photo', 1), ('casual', 1), ('heel', 1), ('check', 1), ('fade', 1), ('chart', 1), ('like', 1), ('measurement', 1), ('practically', 1), ('distressed', 1), ('vintage', 1), ('wrap', 1)], [('definitely', 1), ('dress', 1), ('love', 1), ('fit', 2), ('top', 1), ('get', 1), ('great', 1), ('cardigan', 1), ('shirt', 1), ('style', 1), ('person', 1), ('fabric', 1), ('type', 1), ('much', 1), ('thick', 1), ('cut', 1), ('reference', 1), ('scarf', 1), ('variety', 1), ('drapery', 1), ('swingy', 1), ('cooler_weather', 1)], [('find', 1), ('little', 1), ('get', 1), ('material', 1), ('case', 1), ('first', 1), ('m', 1), ('stiff', 1)], [('dress', 2), ('go', 1), ('sure', 1), ('area', 1), ('back', 1), ('material', 1), ('chest', 1), ('perhaps', 1), ('think', 1), ('beautiful', 1), ('mean', 1), ('however', 1), ('quite', 1), ('odd', 1), ('close', 1), ('wrap', 1)]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjFNNXzAJME9"
      },
      "source": [
        "Gensim creates a unique id for each word in the document. The produced corpus shown above is a mapping of (word_id, word_frequency).\n",
        "\n",
        "For example, (0, 1) above implies, word id 0 occurs once in the first document. Likewise, word id 1 occurs twice and so on.\n",
        "\n",
        "This is used as the input by the LDA model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vl0gK6hEJME-"
      },
      "source": [
        "If you want to see what word a given id corresponds to, pass the id as a key to the dictionary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUb3j6Q3JME_"
      },
      "source": [
        "Or, you can see a human readable form of the corpus itself."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "speeches_corpus = dict(id2word)"
      ],
      "metadata": {
        "id": "sHg-aULmmoSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpqMJzvFJMFA"
      },
      "source": [
        "We are ready to build the topic model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAuJo71iJMFA"
      },
      "source": [
        "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'> 12. Building the Topic Model</h2>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJ0KiNu8JMFA"
      },
      "source": [
        "We have everything required to train the LDA model. In addition to the corpus and dictionary, you need to provide the number of topics as well.\n",
        "\n",
        "Apart from that, `alpha` and `beta` are hyperparameters that affect sparsity of the topics. According to the gensim docs, both default to 1.0/num_topics prior.\n",
        "\n",
        "`chunksize` is the number of documents to be used in each training chunk.  `update_every` determines how often the model parameters should be updated and `passes` is the total number of training passes.\n",
        "\n",
        "The next cell can take about 5 minutes to process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAPlP5krJMFA"
      },
      "source": [
        "# Build LDA model\n",
        "num_topics = 10\n",
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=num_topics,\n",
        "                                           random_state=100,\n",
        "                                           update_every=1,\n",
        "                                           chunksize=100,\n",
        "                                           passes=10,\n",
        "                                           alpha='auto',\n",
        "                                           per_word_topics=True)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVvKis8MJMFA"
      },
      "source": [
        "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'> 13. View the topics in LDA model</h2>\n",
        "</div>\n",
        "\n",
        "The above LDA model is built with 5 different topics where each topic is a combination of keywords and each keyword contributes a certain weightage to the topic.\n",
        "\n",
        "You can see the keywords for each topic and the weightage(importance) of each keyword using `lda_model.print_topics()` as shown next."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xrMa9_nJMFB"
      },
      "source": [
        "# Print the Keywords in the topics\n",
        "## pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpus]"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sMlpj7hK9zn",
        "outputId": "85fe29e4-88cf-4870-ecd0-303de22d8554"
      },
      "source": [
        "!pip install pyLDAvis==3.2.1\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyLDAvis==3.2.1\n",
            "  Downloading pyLDAvis-3.2.1.tar.gz (1.7 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.7 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.6/1.7 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis==3.2.1) (0.41.3)\n",
            "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis==3.2.1) (1.23.5)\n",
            "Requirement already satisfied: scipy>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis==3.2.1) (1.11.3)\n",
            "Requirement already satisfied: joblib>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis==3.2.1) (1.3.2)\n",
            "Requirement already satisfied: jinja2>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis==3.2.1) (3.1.2)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from pyLDAvis==3.2.1) (2.8.7)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyLDAvis==3.2.1) (0.18.3)\n",
            "Collecting funcy (from pyLDAvis==3.2.1)\n",
            "  Downloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: pandas>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis==3.2.1) (1.5.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.7.2->pyLDAvis==3.2.1) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.17.0->pyLDAvis==3.2.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.17.0->pyLDAvis==3.2.1) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=0.17.0->pyLDAvis==3.2.1) (1.16.0)\n",
            "Building wheels for collected packages: pyLDAvis\n",
            "  Building wheel for pyLDAvis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyLDAvis: filename=pyLDAvis-3.2.1-py2.py3-none-any.whl size=136162 sha256=e036816a1880ada3d49b53838923e31593c0561c0a9798ab38505213ef752571\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/35/9c/c084bb1c3a4b8a7feb32d57c88f572dcf5c3accc949478893f\n",
            "Successfully built pyLDAvis\n",
            "Installing collected packages: funcy, pyLDAvis\n",
            "Successfully installed funcy-2.0 pyLDAvis-3.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting tools\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim  # don't skip this"
      ],
      "metadata": {
        "id": "nxHEbyEQJ6-W"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print ([itm for itm in dir(doc_lda) if not itm.startswith('__')])\n",
        "print ([itm for itm in dir(doc_lda.obj) if (not itm.startswith('__')) and (not itm.startswith('_'))])\n",
        "doc_lda.obj.print_topics(num_topics=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuffmcjtIeqF",
        "outputId": "3aa6f88c-828f-4276-d4cb-ed8b816b6b6c"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['_adapt_by_suffix', '_load_specials', '_save_specials', '_smart_save', 'add_lifecycle_event', 'chunksize', 'corpus', 'load', 'metadata', 'obj', 'save', 'save_corpus']\n",
            "['add_lifecycle_event', 'alpha', 'bound', 'callbacks', 'chunksize', 'clear', 'decay', 'diff', 'dispatcher', 'distributed', 'do_estep', 'do_mstep', 'dtype', 'eta', 'eval_every', 'expElogbeta', 'gamma_threshold', 'get_document_topics', 'get_term_topics', 'get_topic_terms', 'get_topics', 'id2word', 'inference', 'init_dir_prior', 'iterations', 'lifecycle_events', 'load', 'log_perplexity', 'minimum_phi_value', 'minimum_probability', 'num_terms', 'num_topics', 'num_updates', 'numworkers', 'offset', 'optimize_alpha', 'optimize_eta', 'passes', 'per_word_topics', 'print_topic', 'print_topics', 'random_state', 'save', 'show_topic', 'show_topics', 'state', 'sync_state', 'top_topics', 'update', 'update_alpha', 'update_eta', 'update_every']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  '0.478*\"dress\" + 0.027*\"knee\" + 0.024*\"lace\" + 0.019*\"easily\" + 0.014*\"curve\" + 0.012*\"wedding\" + 0.012*\"cool\" + 0.010*\"substantial\" + 0.010*\"wrinkle\" + 0.010*\"mean\"'),\n",
              " (1,\n",
              "  '0.214*\"sweater\" + 0.064*\"wide\" + 0.041*\"coat\" + 0.040*\"disappoint\" + 0.039*\"inch\" + 0.030*\"slim\" + 0.024*\"new\" + 0.022*\"staple\" + 0.021*\"suit\" + 0.018*\"couple\"'),\n",
              " (2,\n",
              "  '0.091*\"skirt\" + 0.083*\"jean\" + 0.057*\"pair\" + 0.036*\"casual\" + 0.030*\"wash\" + 0.027*\"comfy\" + 0.025*\"add\" + 0.025*\"warm\" + 0.016*\"heel\" + 0.016*\"neckline\"'),\n",
              " (3,\n",
              "  '0.057*\"time\" + 0.047*\"first\" + 0.045*\"many\" + 0.043*\"compliment\" + 0.041*\"receive\" + 0.031*\"know\" + 0.024*\"embroidery\" + 0.023*\"knit\" + 0.023*\"adorable\" + 0.019*\"feminine\"'),\n",
              " (4,\n",
              "  '0.042*\"love\" + 0.038*\"wear\" + 0.033*\"top\" + 0.028*\"color\" + 0.025*\"great\" + 0.021*\"fabric\" + 0.020*\"get\" + 0.018*\"buy\" + 0.018*\"look\" + 0.016*\"little\"'),\n",
              " (5,\n",
              "  '0.068*\"actually\" + 0.062*\"glad\" + 0.042*\"issue\" + 0.039*\"cardigan\" + 0.032*\"seam\" + 0.031*\"due\" + 0.031*\"texture\" + 0.028*\"clothing\" + 0.026*\"base\" + 0.024*\"similar\"'),\n",
              " (6,\n",
              "  '0.102*\"jacket\" + 0.079*\"thick\" + 0.078*\"expect\" + 0.065*\"heavy\" + 0.044*\"wardrobe\" + 0.043*\"agree\" + 0.036*\"skin\" + 0.029*\"reason\" + 0.028*\"leave\" + 0.026*\"pilcro\"'),\n",
              " (7,\n",
              "  '0.041*\"blue\" + 0.040*\"regular\" + 0.032*\"take\" + 0.031*\"hip\" + 0.030*\"sheer\" + 0.029*\"low\" + 0.027*\"like\" + 0.027*\"tank\" + 0.025*\"happy\" + 0.024*\"boot\"'),\n",
              " (8,\n",
              "  '0.053*\"stretchy\" + 0.044*\"touch\" + 0.039*\"tunic\" + 0.035*\"version\" + 0.032*\"tailor\" + 0.030*\"simple\" + 0.030*\"dressy\" + 0.027*\"subtle\" + 0.026*\"product\" + 0.025*\"addition\"'),\n",
              " (9,\n",
              "  '0.063*\"size\" + 0.059*\"fit\" + 0.035*\"small\" + 0.030*\"look\" + 0.029*\"order\" + 0.026*\"well\" + 0.023*\"make\" + 0.022*\"try\" + 0.021*\"large\" + 0.019*\"run\"')]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9ZOlWfRJMFB"
      },
      "source": [
        "How to interpret this?\n",
        "\n",
        "|Topic | representation\n",
        ":-----:|:--------------:|\n",
        "0 | '0.291*\"thank\" + 0.075*\"make\" + 0.059*\"century\" + 0.052*\"strength\" + 0.050*\"first\" + 0.017*\"freedom\" + 0.007*\"do\" + 0.001*\"high\" |\n",
        "1 | '0.080*\"take\" + 0.005*\"ceremony\"\n",
        "2 | 0.202*\"time\" + 0.000*\"troop\"\n",
        "3 | ???\n",
        "4 | 0.067*\"land\" + 0.011*\"principle\"\n",
        "5 | 0.225*\"people\" + 0.022*\"use\"\n",
        "6 | 0.336*\"nation\"\n",
        "7 | 0.022*\"protect\" + 0.021*\"bless\" + 0.015*\"story\" + 0.013*\"day\" + 0.012*\"world\" + 0.011*\"stand\" + 0.011*\"child\" + 0.010*\"let\" + 0.010*\"democracy\" + 0.009*\"know\"\n",
        "8 | ???\n",
        "9 | 0.179*\"today\" + 0.158*\"great\" + 0.082*\"year\" + 0.017*\"government\" + 0.000*\"troop\"\n",
        "\n",
        "<hr/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqNrGJbJJMFC"
      },
      "source": [
        "## 14. Visualize the topics-keywords\n",
        "\n",
        "Now that the LDA model is built, the next step is to examine the produced topics and the associated keywords. There is no better tool than pyLDAvis package’s interactive chart and is designed to work well with jupyter notebooks.\n",
        "\n",
        "Keep in mind that [Topic 1 in pyLDAvis will be Topic 0 from gensim](\n",
        "https://stackoverflow.com/a/62228381/653651)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwiiQ6bEJMFD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 951
        },
        "outputId": "e5aa3ea2-906d-41d4-9ed1-32326965e152"
      },
      "source": [
        "# Visualize the topics\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
        "vis"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/pyLDAvis/_prepare.py:248: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
            "  by='saliency', ascending=False).head(R).drop('saliency', 1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
              "topic                                                \n",
              "4      0.340048 -0.005329       1        1  43.912433\n",
              "9      0.335314 -0.004820       2        1  30.531664\n",
              "7     -0.062712 -0.253809       3        1   5.150155\n",
              "2     -0.060939  0.390952       4        1   4.849363\n",
              "0     -0.059613 -0.024164       5        1   4.484804\n",
              "3     -0.069015 -0.026319       6        1   4.171569\n",
              "1     -0.098682 -0.020284       7        1   2.157317\n",
              "5     -0.105690 -0.019441       8        1   1.786140\n",
              "6     -0.108188 -0.018613       9        1   1.632478\n",
              "8     -0.110524 -0.018173      10        1   1.324077, topic_info=           Term          Freq         Total Category  logprob  loglift\n",
              "7         dress  12148.000000  12148.000000  Default  30.0000  30.0000\n",
              "48         size  10887.000000  10887.000000  Default  29.0000  29.0000\n",
              "30          fit  10530.000000  10530.000000  Default  28.0000  28.0000\n",
              "352     sweater   2614.000000   2614.000000  Default  27.0000  27.0000\n",
              "15         love  10475.000000  10475.000000  Default  26.0000  26.0000\n",
              "...         ...           ...           ...      ...      ...      ...\n",
              "1996        lie     90.955433     91.845532  Topic10  -4.4127   4.3147\n",
              "290     provide     86.959333     87.849439  Topic10  -4.4576   4.3143\n",
              "1030  exception     84.428217     85.318312  Topic10  -4.4872   4.3140\n",
              "1211     overly     80.898838     81.788914  Topic10  -4.5299   4.3135\n",
              "976      modest     80.313171     81.203245  Topic10  -4.5371   4.3134\n",
              "\n",
              "[348 rows x 6 columns], token_table=      Topic      Freq        Term\n",
              "term                             \n",
              "506       8  0.998553    actually\n",
              "507       4  0.998136         add\n",
              "1220     10  0.996517    addition\n",
              "67       10  0.995176  adjustable\n",
              "491       6  0.997602    adorable\n",
              "...     ...       ...         ...\n",
              "55        2  0.023949        work\n",
              "1097      9  0.993271       worry\n",
              "857       5  0.995732     wrinkle\n",
              "1297      7  0.988057       young\n",
              "57        6  0.998167      zipper\n",
              "\n",
              "[334 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[5, 10, 8, 3, 1, 4, 2, 6, 7, 9])"
            ],
            "text/html": [
              "\n",
              "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
              "\n",
              "\n",
              "<div id=\"ldavis_el5881367380854545609597299477\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "\n",
              "var ldavis_el5881367380854545609597299477_data = {\"mdsDat\": {\"x\": [0.34004835280533136, 0.3353144581315539, -0.06271239907236174, -0.06093892242512215, -0.05961310107679358, -0.0690146405765529, -0.09868158014494377, -0.10568995407612199, -0.10818830424021988, -0.11052390932476909], \"y\": [-0.0053289730484724586, -0.004819871129924256, -0.2538092351691318, 0.39095155067141135, -0.024163861893430526, -0.026319375666008133, -0.020283766097368452, -0.019440657265163617, -0.018612595313117243, -0.018173215088794884], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [43.91243345647242, 30.53166401353437, 5.150155054843418, 4.849362834032653, 4.484804286992257, 4.171569202433107, 2.157316590518276, 1.786139835361107, 1.632477726010884, 1.3240769998015056]}, \"tinfo\": {\"Term\": [\"dress\", \"size\", \"fit\", \"sweater\", \"love\", \"skirt\", \"small\", \"jean\", \"top\", \"order\", \"wear\", \"color\", \"well\", \"great\", \"pair\", \"try\", \"make\", \"time\", \"fabric\", \"large\", \"jacket\", \"get\", \"run\", \"look\", \"blue\", \"first\", \"regular\", \"many\", \"see\", \"buy\", \"love\", \"color\", \"great\", \"fabric\", \"little\", \"really\", \"soft\", \"perfect\", \"long\", \"nice\", \"comfortable\", \"material\", \"length\", \"short\", \"shirt\", \"bit\", \"cute\", \"back\", \"also\", \"waist\", \"sleeve\", \"good\", \"cut\", \"return\", \"pant\", \"still\", \"style\", \"fall\", \"right\", \"flatter\", \"top\", \"get\", \"buy\", \"wear\", \"work\", \"quality\", \"flattering\", \"look\", \"size\", \"small\", \"order\", \"try\", \"large\", \"run\", \"petite\", \"purchase\", \"medium\", \"find\", \"big\", \"way\", \"even\", \"store\", \"retailer\", \"usually\", \"true\", \"perfectly\", \"model\", \"say\", \"online\", \"review\", \"come\", \"detail\", \"gorgeous\", \"person\", \"piece\", \"thin\", \"legging\", \"chest\", \"well\", \"fit\", \"see\", \"make\", \"need\", \"much\", \"think\", \"beautiful\", \"look\", \"go\", \"blue\", \"regular\", \"take\", \"hip\", \"sheer\", \"low\", \"like\", \"tank\", \"happy\", \"boot\", \"boxy\", \"full\", \"flow\", \"put\", \"green\", \"read\", \"consider\", \"appear\", \"snug\", \"less\", \"available\", \"pound\", \"walk\", \"roomy\", \"waiste\", \"let\", \"underneath\", \"second\", \"complaint\", \"half\", \"skirt\", \"jean\", \"pair\", \"casual\", \"wash\", \"comfy\", \"add\", \"warm\", \"heel\", \"neckline\", \"ankle\", \"frame\", \"versatile\", \"flare\", \"cream\", \"hole\", \"bootie\", \"grey\", \"slit\", \"cozy\", \"roll\", \"odd\", \"close\", \"amount\", \"throw\", \"outfit\", \"rise\", \"collar\", \"point\", \"become\", \"dress\", \"knee\", \"lace\", \"easily\", \"curve\", \"wedding\", \"cool\", \"substantial\", \"wrinkle\", \"mean\", \"weather\", \"depend\", \"totally\", \"bodice\", \"rich\", \"closet\", \"broad_shoulder\", \"lining\", \"classy\", \"hour\", \"snag\", \"difficult\", \"edge\", \"linen\", \"truly\", \"cuff\", \"bring\", \"clingy\", \"sizing\", \"bead\", \"time\", \"first\", \"many\", \"compliment\", \"receive\", \"know\", \"embroidery\", \"knit\", \"adorable\", \"feminine\", \"favorite\", \"extremely\", \"jumpsuit\", \"zipper\", \"arrive\", \"place\", \"send\", \"hard\", \"several\", \"cotton\", \"exchange\", \"sadly\", \"special\", \"rather\", \"ever\", \"eye\", \"highly_recommend\", \"hide\", \"beading\", \"busty\", \"sweater\", \"wide\", \"coat\", \"disappoint\", \"inch\", \"slim\", \"new\", \"staple\", \"suit\", \"couple\", \"last\", \"unflattering\", \"turn\", \"chic\", \"month\", \"wool\", \"scratchy\", \"move\", \"hug\", \"live\", \"construction\", \"money\", \"hemline\", \"mid\", \"week\", \"name\", \"young\", \"forgive\", \"cashmere\", \"proportion\", \"actually\", \"glad\", \"issue\", \"cardigan\", \"seam\", \"due\", \"texture\", \"clothing\", \"base\", \"similar\", \"sometimes\", \"excellent\", \"clothe\", \"romantic\", \"mail\", \"dd\", \"stitch\", \"sweatshirt\", \"sleeveless\", \"previous\", \"shapeless\", \"shrink\", \"create\", \"buying\", \"appreciate\", \"change\", \"garment\", \"expectation\", \"flair\", \"disappointing\", \"jacket\", \"thick\", \"expect\", \"heavy\", \"wardrobe\", \"agree\", \"skin\", \"reason\", \"leave\", \"pilcro\", \"slimme\", \"interesting\", \"state\", \"stiff\", \"bulky\", \"thread\", \"movement\", \"worry\", \"bell\", \"flimsy\", \"width\", \"possible\", \"sew\", \"looking\", \"certainly\", \"beige\", \"lounge\", \"image\", \"beauty\", \"ridiculous\", \"stretchy\", \"touch\", \"tunic\", \"version\", \"tailor\", \"simple\", \"dressy\", \"subtle\", \"product\", \"addition\", \"motif\", \"form_fitte\", \"adjustable\", \"sock\", \"fairly\", \"event\", \"coverage\", \"sack\", \"airy\", \"recently\", \"leather\", \"frumpy\", \"cinch\", \"opt\", \"avoid\", \"lie\", \"provide\", \"exception\", \"overly\", \"modest\"], \"Freq\": [12148.0, 10887.0, 10530.0, 2614.0, 10475.0, 2514.0, 6023.0, 2288.0, 8255.0, 4972.0, 10027.0, 6994.0, 4486.0, 6331.0, 1562.0, 3789.0, 4084.0, 1341.0, 5266.0, 3627.0, 946.0, 4901.0, 3309.0, 9535.0, 1210.0, 1113.0, 1175.0, 1073.0, 3080.0, 4548.0, 10474.843432720478, 6993.245985263254, 6330.6833125312, 5265.1826833606965, 4050.1839514749477, 3555.196570454748, 3535.898075037801, 3537.4609764607167, 3455.8102692199595, 3272.0270594007675, 3206.0220521694846, 3111.0025586915176, 3069.6855539191943, 2963.4661938981258, 2957.085041662066, 2885.2886781794978, 2777.953688346757, 2596.599650045211, 2596.418655678174, 2511.497072001617, 2458.436985838133, 2408.5075692064056, 2052.4587411779703, 1964.432907121823, 1932.980674720386, 1922.557044582394, 1852.9683025343895, 1845.0391965536135, 1797.503262409412, 1741.9924522620404, 8245.455397734991, 4892.870086210255, 4498.553579762214, 9487.133301864054, 3015.4868949961033, 2238.978418968958, 2118.6306062124113, 4414.420167458631, 10886.123817720476, 6022.712399787654, 4971.308723583992, 3788.6619066965927, 3626.8658985874317, 3308.7687450649887, 2743.7894827512955, 2392.275049917185, 2357.1971625783126, 2256.2543982683196, 2251.787707493806, 2245.4894495307385, 2243.7392531022992, 2129.25977804386, 2123.4065617430415, 1926.414235584743, 1796.4623924684447, 1620.733776665093, 1581.7929534787338, 1559.897290408602, 1520.863978477083, 1390.9520956548608, 1297.814237411795, 1269.518482080789, 1236.0736495018352, 1235.704500172287, 1232.1142772188464, 1137.5683020002573, 1021.6127935793877, 988.5746536707877, 4478.22898716776, 10265.414769150893, 3016.715068685674, 3952.372786317039, 1294.253742983584, 2715.0009912134983, 2603.6498593384645, 2607.895318323598, 5120.5746014168535, 2657.549367447545, 1210.0249697141328, 1174.1299742541269, 920.7494363541808, 908.216868541548, 877.4806058690139, 837.9859129887086, 799.1188025112629, 796.7602994612098, 741.9352552160525, 711.5750527481941, 691.3304240368108, 611.0090088955449, 575.7326808523377, 514.3738215795412, 454.7063362932124, 388.8090967995471, 387.5865477964887, 384.85038093146227, 374.2126825340634, 354.9922172060559, 344.7505561924623, 307.66774755923126, 300.1066706469634, 270.57773626957936, 267.9257453552527, 262.9954319057466, 260.2125700832446, 249.24346795097924, 231.059532831523, 230.2963230492669, 2513.9218438695575, 2287.2611212043303, 1561.7500542206167, 992.4606936903722, 831.250156210859, 736.2125336008272, 695.4154154637779, 685.5348510186371, 440.4956550543187, 436.0705736555416, 416.8061934607958, 416.79360199463383, 406.7866366320666, 397.14784369268193, 344.64624117059367, 306.4710143452508, 299.5041458808788, 290.60087883071054, 286.6623603479314, 269.56266304419444, 268.79041204533894, 259.7955753489767, 248.6646168921681, 247.09582769651473, 239.35194804486017, 236.4999553667673, 224.27082655831373, 210.12600342448067, 197.70590404590416, 193.5141486572866, 12147.241814473702, 689.3749736330122, 617.436296506319, 480.9745550410843, 355.44600933867986, 306.322000619341, 302.24214969005095, 253.9451169908391, 251.19150459365545, 248.1314803030384, 240.20469976712783, 230.71582448730305, 217.08023487644647, 186.57810957449553, 186.05803065124823, 174.42092303018714, 167.77668904775803, 160.03925619819594, 149.91373482571558, 140.5806216863732, 136.49393057191386, 135.39312229463576, 135.263596642943, 133.88226976833963, 131.40490381130803, 131.39260804592578, 130.77533458842973, 130.20106672419556, 127.22517197746922, 124.92079880037777, 1340.9907454198196, 1112.5800572305152, 1072.8697029649009, 1019.2879048811214, 960.0335387108075, 727.3713304853349, 566.3178531575755, 555.3982740709163, 535.40501654256, 455.6511714447741, 452.03840533023543, 420.58469512500176, 403.2711043035073, 364.7892296539694, 361.4527784381131, 349.09167700774225, 342.61233370193315, 322.06624823788957, 309.57160157802247, 305.9678837802202, 295.01484133239467, 290.3598676404818, 286.9400662368786, 283.1165249777311, 275.78087187197383, 259.81140740090194, 257.2929812579753, 254.82635727874884, 254.75773537298892, 237.8890971781592, 2613.6193731816243, 787.6152764864552, 497.19498350293316, 483.8676129991628, 472.51894352775673, 366.4545750273443, 299.21974939276, 267.9471057238624, 261.4873065112853, 216.78555425244045, 208.92567229484058, 198.6857840159815, 169.21683957521535, 169.0270142494736, 162.63962486546467, 154.66909021543486, 149.12458083746188, 142.94592758505186, 137.69175201813505, 128.8040034358552, 126.81318855795683, 124.647337720246, 115.51624201912708, 105.4984560886498, 104.40757241976779, 85.58799042285585, 85.14218610645172, 80.65745035702072, 75.45969411486331, 72.84653611524975, 687.110991070134, 624.691550416972, 423.04753967644166, 391.0759656695068, 321.29461022901455, 316.5999657071185, 310.5315865058939, 278.85763898807704, 265.9260536532651, 242.05547413549687, 239.82004224085748, 209.64961091570834, 189.09168378803204, 184.4456979073226, 170.8915421316326, 168.23792117120402, 163.4173787780005, 153.72641935569303, 136.30457087407387, 135.62058798698678, 125.99982633703823, 124.95693648304274, 123.185882600689, 111.74711459032576, 111.06607271370947, 95.18367353986913, 92.77826632212293, 88.68129734157489, 88.58430055393241, 84.30530941892603, 945.3576780960256, 729.2530505339123, 722.9349538812145, 601.1044971237695, 407.60187146485345, 395.9150916077436, 329.1008180293598, 266.1476245762997, 259.59581024642307, 244.0305989147623, 209.60777927397956, 201.58063502996274, 188.95179924802292, 187.73856603010546, 182.91285771829567, 171.75049525317456, 140.05698943056927, 136.04242654135075, 133.94881481127373, 118.42151970997504, 115.1514279177917, 105.66773525979126, 102.434198469977, 96.76631218348226, 95.80226984195515, 82.28773986638946, 72.57381875486323, 72.08703328612417, 62.84195614146718, 51.136545553123966, 400.8316326729388, 329.92606592844794, 292.46310539119406, 259.8504248680177, 236.86568727319863, 228.1567783099468, 221.58884393991843, 203.42443669696246, 191.70100967958504, 190.77755943204377, 189.2407909734524, 167.7166968675178, 146.82227239810737, 139.35227041384996, 137.35342744205477, 137.23754195484895, 133.9438477680143, 132.3961923346842, 131.34177695199995, 126.50129381762959, 120.6750433364113, 120.64506572304221, 120.28102988479603, 94.55084602615217, 91.1110022394074, 90.95543310037802, 86.95933338481434, 84.4282165748955, 80.8988382519973, 80.31317072069112], \"Total\": [12148.0, 10887.0, 10530.0, 2614.0, 10475.0, 2514.0, 6023.0, 2288.0, 8255.0, 4972.0, 10027.0, 6994.0, 4486.0, 6331.0, 1562.0, 3789.0, 4084.0, 1341.0, 5266.0, 3627.0, 946.0, 4901.0, 3309.0, 9535.0, 1210.0, 1113.0, 1175.0, 1073.0, 3080.0, 4548.0, 10475.72054802556, 6994.123094283568, 6331.56042327547, 5266.059798837954, 4051.061063869466, 3556.0736752015405, 3536.7751831097066, 3538.3418959924334, 3456.6873803590274, 3272.9041661893048, 3206.899158927729, 3111.879680210836, 3070.562681106206, 2964.3432880675573, 2957.9621292672, 2886.1657937196537, 2778.830784036726, 2597.4767495969627, 2597.2957672134626, 2512.3742025183547, 2459.3141400845125, 2409.384687985342, 2053.335868833664, 1965.3100288763362, 1933.857766140693, 1923.4341626488279, 1853.845410050103, 1845.9163026121494, 1798.3803864858255, 1742.8695766454225, 8255.139302774765, 4901.612732252366, 4548.856833257013, 10027.714063289053, 3089.9501147543997, 2275.7686388851735, 2168.8627057084573, 9535.77125618493, 10887.00317691169, 6023.591757877184, 4972.188071131285, 3789.5412740879215, 3627.745246321796, 3309.648089653712, 2744.668848525057, 2393.1544126772865, 2358.0765062474215, 2257.133773772255, 2252.6670602061004, 2246.3688263837644, 2244.6185997053985, 2130.1391193933096, 2124.2859117211374, 1927.2935811914929, 1797.3417612710207, 1621.613144066158, 1582.6723250516702, 1560.7766473649767, 1521.7433402316128, 1391.831495764191, 1298.6936066544322, 1270.397829896601, 1236.9530177226468, 1236.5838387487581, 1232.993631620271, 1138.4476885636657, 1022.4921978722467, 989.4539966614974, 4486.083979619569, 10530.626365699514, 3080.645321808469, 4084.6216690547244, 1300.1966539233722, 3057.6503215316125, 3089.165763682411, 3107.507054468594, 9535.77125618493, 4407.9858876050885, 1210.901636393678, 1175.0066618072221, 921.6260948013787, 909.0935488111099, 878.3573544912649, 838.8626144644769, 799.9954985640077, 797.6369457347282, 742.811953778489, 712.4517985997403, 692.2070967499047, 611.8857055137323, 576.6094017524566, 515.2505437684283, 455.58299413037713, 389.6857912223111, 388.46329218831806, 385.7270716432866, 375.0893881605822, 355.8689238993248, 345.62729085983443, 308.54443872068896, 300.98343946142086, 271.45444427027485, 268.8024215211268, 263.8722239790992, 261.08926006737823, 250.12016883507397, 231.93627742741617, 231.17314814244827, 2514.804309781005, 2288.143555928465, 1562.632521204307, 993.343162942316, 832.132590962938, 737.0949996048118, 696.2978829350329, 686.417349512906, 441.3781645428424, 436.95309981077406, 417.688667386827, 417.6760654863828, 407.66907684193626, 398.0303638091957, 345.5287778286773, 307.3534921519871, 300.3866368422331, 291.4833423515367, 287.54486835822706, 270.44514977648504, 269.6729540118724, 260.6780843806198, 249.54705879637405, 247.9783265524788, 240.2344015177839, 237.3823905912083, 225.15325985545982, 211.0085223514117, 198.58834589824826, 194.3966803044462, 12148.126295882299, 690.2594654894276, 618.3208105843937, 481.8590252485973, 356.330530255406, 307.2064548382184, 303.1266056553904, 254.8296573078796, 252.07596021549804, 249.0159793515361, 241.08922082929973, 231.6003225626091, 217.964729459431, 187.46260042837127, 186.9425080874775, 175.30540069975663, 168.66120819782324, 160.92369501962744, 150.79823556947179, 141.46512480957367, 137.37842461832994, 136.27764569048554, 136.14812357389292, 134.76671849283395, 132.28934644112374, 132.27716327982964, 131.6598478739269, 131.0855555675783, 128.1096170778966, 125.80537061111451, 1341.8716761461403, 1113.46097982185, 1073.7506220679443, 1020.168804223883, 960.9144392788269, 728.2522656883368, 567.1988176357127, 556.279254617181, 536.2859513740254, 456.5321670683287, 452.91934516341917, 421.4656400836979, 404.1521451778934, 365.67020061110804, 362.33369180891987, 349.9726226293106, 343.49326056851885, 322.94718245366255, 310.45261343044785, 306.84881337036006, 295.89578775394233, 291.24079194923763, 287.8210166747535, 283.9974721911845, 276.6618194688318, 260.6923416259957, 258.1738787125742, 255.70729305668456, 255.63879735454387, 238.77003377636905, 2614.5044801661757, 788.5004005172485, 498.0800849414851, 484.75273395240646, 473.4040623159959, 367.3397399164657, 300.10488959265194, 268.8322173948507, 262.37236886132257, 217.67067081607888, 209.81079540178982, 199.57087495197862, 170.10197716625356, 169.91221733696565, 163.52476446145062, 155.5541999532146, 150.0097026497041, 143.83101819449445, 138.5768480036857, 129.6891094021673, 127.69834698910162, 125.53248011869209, 116.40157406422524, 106.3836134962922, 105.29266085653852, 86.47396426762995, 86.02738471109659, 81.54256069818769, 76.34478404836331, 73.73163308103378, 687.9956216206907, 625.5761962851134, 423.93213101129703, 391.9605513893025, 322.17925799780267, 317.48457727753384, 311.41618746477724, 279.74227894100113, 266.8107081998284, 242.94012510360577, 240.70464960546425, 210.53424941825827, 189.9763021790452, 185.33041151547454, 171.77615026561685, 169.12256353712937, 164.30199004494747, 154.61102906586092, 137.18938753075562, 136.50520990899258, 126.8844466661312, 125.84153186089554, 124.07049625212915, 112.6318172515716, 111.95077300615593, 96.06824912058063, 93.6628390981392, 89.56602179978482, 89.4690511428894, 85.18994947815816, 946.2365523270827, 730.1319658753157, 723.8138504834501, 601.983453157766, 408.4808028411807, 396.7940132481761, 329.9797849582646, 267.0265719169479, 260.4747111769193, 244.90953720835006, 210.48677370517206, 202.45956111455573, 189.830796287043, 188.6174887841876, 183.79181686162798, 172.62939941993628, 140.93592019798817, 136.92138991261103, 134.82784442015566, 119.30049530635306, 116.03042901432414, 106.54675161251802, 103.3130457047815, 97.645464991458, 96.681416041177, 83.1666845686923, 73.45270460059622, 72.96593367086453, 63.72097286424064, 52.0154563152967, 401.7217006173337, 330.8161481372732, 293.35310492607016, 260.7405133537028, 237.7557596203334, 229.0468576814564, 222.47894185399193, 204.31456889873326, 192.5910803738578, 191.66761682268535, 190.13084512234147, 168.6068082764405, 147.71252068061457, 140.2424678626797, 138.24341515376622, 138.12757339467916, 134.83389521905713, 133.28625160622684, 132.2319534414001, 127.39138749454979, 121.56511395895811, 121.53513371734893, 121.17120342239389, 95.44102274999601, 92.00114413566443, 91.84553163109707, 87.84943912035946, 85.31831168187041, 81.78891368765206, 81.2032449659509], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.1678, -3.5719, -3.6714, -3.8557, -4.118, -4.2484, -4.2538, -4.2534, -4.2767, -4.3314, -4.3518, -4.3819, -4.3952, -4.4304, -4.4326, -4.4572, -4.4951, -4.5626, -4.5627, -4.5959, -4.6173, -4.6378, -4.7978, -4.8416, -4.8577, -4.8631, -4.9, -4.9043, -4.9304, -4.9618, -3.4071, -3.929, -4.013, -3.2669, -4.413, -4.7108, -4.766, -4.0319, -2.7659, -3.3578, -3.5497, -3.8213, -3.865, -3.9568, -4.144, -4.2811, -4.2959, -4.3397, -4.3416, -4.3444, -4.3452, -4.3976, -4.4003, -4.4977, -4.5675, -4.6705, -4.6948, -4.7087, -4.7341, -4.8234, -4.8927, -4.9147, -4.9414, -4.9417, -4.9446, -5.0245, -5.132, -5.1649, -3.6541, -2.8246, -4.0492, -3.779, -4.8954, -4.1546, -4.1964, -4.1948, -3.5201, -4.176, -3.183, -3.2131, -3.4562, -3.4699, -3.5043, -3.5504, -3.5979, -3.6008, -3.6721, -3.7139, -3.7428, -3.8663, -3.9257, -4.0384, -4.1617, -4.3183, -4.3214, -4.3285, -4.3566, -4.4093, -4.4386, -4.5524, -4.5772, -4.6808, -4.6907, -4.7092, -4.7199, -4.763, -4.8387, -4.842, -2.3916, -2.4861, -2.8676, -3.321, -3.4983, -3.6197, -3.6767, -3.691, -4.1333, -4.1434, -4.1886, -4.1886, -4.2129, -4.2369, -4.3787, -4.4961, -4.5191, -4.5492, -4.5629, -4.6244, -4.6273, -4.6613, -4.7051, -4.7114, -4.7433, -4.7553, -4.8083, -4.8735, -4.9344, -4.9559, -0.7382, -3.6073, -3.7175, -3.9672, -4.2697, -4.4184, -4.4318, -4.6059, -4.6168, -4.6291, -4.6616, -4.7019, -4.7628, -4.9142, -4.917, -4.9816, -5.0204, -5.0676, -5.133, -5.1973, -5.2268, -5.2349, -5.2358, -5.2461, -5.2648, -5.2649, -5.2696, -5.274, -5.2971, -5.3154, -2.8695, -3.0562, -3.0926, -3.1438, -3.2037, -3.4812, -3.7315, -3.751, -3.7876, -3.9489, -3.9569, -4.029, -4.071, -4.1713, -4.1805, -4.2153, -4.234, -4.2959, -4.3355, -4.3472, -4.3836, -4.3995, -4.4114, -4.4248, -4.451, -4.5107, -4.5204, -4.5301, -4.5303, -4.5988, -1.5427, -2.7422, -3.2022, -3.2294, -3.2531, -3.5073, -3.71, -3.8204, -3.8448, -4.0323, -4.0692, -4.1195, -4.28, -4.2812, -4.3197, -4.3699, -4.4064, -4.4488, -4.4862, -4.5529, -4.5685, -4.5857, -4.6618, -4.7525, -4.7629, -4.9617, -4.9669, -5.021, -5.0876, -5.1229, -2.6899, -2.7852, -3.1749, -3.2535, -3.4501, -3.4648, -3.4841, -3.5917, -3.6392, -3.7332, -3.7425, -3.877, -3.9802, -4.0051, -4.0814, -4.097, -4.1261, -4.1872, -4.3075, -4.3125, -4.3861, -4.3944, -4.4087, -4.5062, -4.5123, -4.6666, -4.6922, -4.7374, -4.7385, -4.788, -2.2809, -2.5404, -2.5491, -2.7337, -3.1222, -3.1513, -3.3361, -3.5484, -3.5733, -3.6352, -3.7872, -3.8263, -3.891, -3.8974, -3.9234, -3.9864, -4.1904, -4.2195, -4.235, -4.3582, -4.3862, -4.4722, -4.5032, -4.5602, -4.5702, -4.7222, -4.8478, -4.8546, -4.9918, -5.198, -2.9295, -3.1242, -3.2447, -3.363, -3.4556, -3.493, -3.5222, -3.6078, -3.6671, -3.672, -3.68, -3.8008, -3.9338, -3.9861, -4.0005, -4.0014, -4.0256, -4.0373, -4.0453, -4.0828, -4.13, -4.1302, -4.1332, -4.3739, -4.411, -4.4127, -4.4576, -4.4872, -4.5299, -4.5371], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.8229, 0.8228, 0.8228, 0.8228, 0.8228, 0.8227, 0.8227, 0.8227, 0.8227, 0.8227, 0.8227, 0.8227, 0.8227, 0.8227, 0.8227, 0.8227, 0.8227, 0.8226, 0.8226, 0.8226, 0.8226, 0.8226, 0.8225, 0.8225, 0.8225, 0.8225, 0.8225, 0.8225, 0.8225, 0.8225, 0.8218, 0.8212, 0.8119, 0.7676, 0.7986, 0.8067, 0.7995, 0.0528, 1.1863, 1.1863, 1.1862, 1.1862, 1.1862, 1.1861, 1.1861, 1.186, 1.186, 1.186, 1.186, 1.186, 1.186, 1.186, 1.186, 1.1859, 1.1859, 1.1859, 1.1859, 1.1858, 1.1858, 1.1858, 1.1857, 1.1857, 1.1857, 1.1857, 1.1857, 1.1856, 1.1855, 1.1855, 1.1847, 1.1609, 1.1654, 1.1535, 1.1818, 1.0676, 1.0154, 1.0111, 0.5646, 0.6804, 2.9654, 2.9654, 2.9652, 2.9652, 2.9651, 2.9651, 2.965, 2.965, 2.965, 2.9649, 2.9649, 2.9647, 2.9646, 2.9644, 2.9642, 2.9639, 2.9639, 2.9639, 2.9638, 2.9637, 2.9636, 2.9633, 2.9632, 2.9629, 2.9629, 2.9628, 2.9628, 2.9626, 2.9624, 2.9623, 3.026, 3.0259, 3.0258, 3.0254, 3.0253, 3.0251, 3.0251, 3.025, 3.0243, 3.0243, 3.0242, 3.0242, 3.0242, 3.0241, 3.0238, 3.0234, 3.0234, 3.0233, 3.0232, 3.0231, 3.023, 3.0229, 3.0228, 3.0228, 3.0226, 3.0226, 3.0224, 3.0221, 3.0219, 3.0218, 3.1044, 3.1032, 3.103, 3.1026, 3.102, 3.1016, 3.1016, 3.101, 3.101, 3.1009, 3.1008, 3.1006, 3.1004, 3.0997, 3.0997, 3.0994, 3.0992, 3.099, 3.0986, 3.0982, 3.098, 3.098, 3.098, 3.0979, 3.0978, 3.0978, 3.0977, 3.0977, 3.0975, 3.0974, 3.1762, 3.1761, 3.1761, 3.176, 3.176, 3.1757, 3.1753, 3.1753, 3.1752, 3.1749, 3.1749, 3.1748, 3.1747, 3.1745, 3.1744, 3.1744, 3.1743, 3.1741, 3.174, 3.174, 3.1739, 3.1738, 3.1738, 3.1738, 3.1737, 3.1735, 3.1735, 3.1734, 3.1734, 3.1732, 3.836, 3.8352, 3.8345, 3.8345, 3.8344, 3.8339, 3.8334, 3.833, 3.8329, 3.8322, 3.8321, 3.8319, 3.8311, 3.8311, 3.8309, 3.8306, 3.8304, 3.8301, 3.8299, 3.8295, 3.8293, 3.8292, 3.8287, 3.8279, 3.8279, 3.826, 3.826, 3.8254, 3.8246, 3.8242, 4.0238, 4.0237, 4.023, 4.0229, 4.0224, 4.0223, 4.0223, 4.0219, 4.0218, 4.0215, 4.0214, 4.0209, 4.0204, 4.0203, 4.02, 4.0199, 4.0197, 4.0194, 4.0186, 4.0186, 4.0181, 4.0181, 4.018, 4.0172, 4.0172, 4.0159, 4.0156, 4.0152, 4.0152, 4.0147, 4.1141, 4.1139, 4.1139, 4.1136, 4.1129, 4.1129, 4.1124, 4.1118, 4.1117, 4.1115, 4.1109, 4.1107, 4.1104, 4.1104, 4.1103, 4.11, 4.1088, 4.1086, 4.1085, 4.1077, 4.1075, 4.1068, 4.1065, 4.106, 4.1059, 4.1044, 4.103, 4.103, 4.1012, 4.098, 4.3222, 4.3218, 4.3214, 4.321, 4.3207, 4.3206, 4.3204, 4.3201, 4.3198, 4.3198, 4.3198, 4.3192, 4.3184, 4.3181, 4.318, 4.318, 4.3178, 4.3178, 4.3177, 4.3174, 4.3171, 4.3171, 4.3171, 4.3151, 4.3147, 4.3147, 4.3143, 4.314, 4.3135, 4.3134]}, \"token.table\": {\"Topic\": [8, 4, 10, 10, 6, 9, 10, 1, 4, 4, 3, 8, 6, 3, 10, 1, 8, 5, 6, 1, 2, 9, 4, 9, 9, 2, 1, 3, 5, 3, 4, 3, 5, 5, 9, 6, 1, 2, 8, 8, 7, 4, 9, 8, 2, 7, 10, 5, 5, 4, 5, 8, 8, 7, 4, 1, 2, 1, 4, 3, 6, 3, 7, 5, 6, 7, 10, 4, 4, 8, 5, 5, 1, 1, 8, 5, 2, 5, 7, 8, 5, 10, 8, 5, 5, 6, 2, 10, 6, 8, 10, 6, 9, 8, 6, 6, 1, 10, 1, 6, 6, 2, 6, 1, 2, 8, 4, 1, 1, 2, 9, 3, 7, 10, 4, 10, 3, 8, 1, 2, 8, 1, 2, 1, 2, 1, 3, 4, 3, 3, 6, 9, 4, 7, 6, 6, 3, 4, 5, 7, 9, 7, 9, 8, 9, 4, 6, 5, 6, 6, 5, 2, 7, 10, 9, 2, 1, 3, 3, 10, 3, 5, 5, 1, 7, 1, 1, 2, 9, 9, 1, 3, 8, 1, 2, 6, 1, 5, 2, 7, 2, 10, 7, 7, 10, 7, 9, 1, 2, 7, 4, 1, 2, 7, 1, 4, 2, 10, 2, 4, 10, 4, 1, 1, 2, 2, 2, 2, 9, 6, 4, 9, 3, 8, 10, 7, 10, 2, 3, 1, 2, 6, 3, 1, 9, 6, 10, 3, 2, 1, 2, 5, 9, 1, 4, 4, 8, 3, 2, 10, 6, 2, 7, 8, 3, 1, 2, 6, 6, 9, 8, 3, 1, 1, 8, 8, 10, 2, 5, 9, 4, 1, 8, 7, 9, 4, 2, 5, 3, 10, 1, 8, 6, 7, 9, 9, 1, 8, 2, 10, 1, 5, 10, 7, 7, 8, 10, 3, 3, 8, 9, 2, 1, 2, 9, 4, 6, 1, 2, 5, 10, 2, 5, 2, 10, 7, 3, 7, 2, 4, 10, 1, 3, 3, 9, 4, 4, 2, 1, 2, 5, 5, 7, 1, 2, 7, 9, 7, 1, 2, 9, 5, 7, 6], \"Freq\": [0.9985528663418739, 0.9981360234364607, 0.9965167990620817, 0.9951763013904882, 0.9976021162390499, 0.9979989283566144, 0.9906833907437808, 0.9995011091036226, 0.9960547900855692, 0.9983512423472356, 0.9981150619265868, 0.9915072224994494, 0.9963191614827164, 0.9981850655997856, 0.9891181338550731, 0.9998164566450742, 0.9969614855217084, 0.9935982811607936, 0.9975011721180259, 0.16057887922810607, 0.8392579499537087, 0.9886854699193514, 0.9979594286084261, 0.9859717316526107, 0.9938599892053759, 0.9997038798063486, 0.9995960752766905, 0.9992554007967457, 0.9975323054981944, 0.9993658538014385, 0.9987128693663022, 0.9982561624179059, 0.9949882376093981, 0.9960796664218857, 0.9956917730334854, 0.9967749982517058, 0.9890397005039802, 0.010991772621737944, 0.9943904194481707, 0.9975493671853001, 0.9823853840818856, 0.9986478359217398, 0.9929519439300849, 0.9888803102964872, 0.9995411644573379, 0.9946312434075498, 0.9903343089008437, 0.994706598744625, 0.9917187247452397, 0.9978077930510877, 0.9925535625568526, 0.9948609265058487, 0.9973465614714689, 0.9978315034587019, 0.995220466262817, 0.9998394231459143, 0.9994659197127959, 0.9997196173365085, 0.9985144389727255, 0.9959632126642665, 0.9988543031123441, 0.9988073720281053, 0.9945312762023363, 0.9962833824732917, 0.9972337733327469, 0.9969188737574776, 0.9938153887959527, 0.9983540108711398, 0.9984696561831979, 0.991371870956704, 0.9903447938543418, 0.9962660222955009, 0.9993494153324158, 0.9997010310805903, 0.9933624259610817, 0.9974079372776053, 0.9996868462089286, 0.99062468621312, 0.9984471795625183, 0.9860318090872533, 0.9999072864526705, 0.9978472485979988, 0.9984736982133459, 0.99821726852962, 0.9915670995401579, 0.9978864243040743, 0.9997244076541646, 0.9918367247974647, 0.9976078395273245, 0.9974624108916507, 0.9845483149410404, 0.9969726241771064, 0.998875608026972, 0.993680395886622, 0.998895188505508, 0.9973442195437064, 0.9997987491827973, 0.9910056102680682, 0.9995036055476336, 0.9979701790766137, 0.99883432733394, 0.9994976931427683, 0.999585993734667, 0.025069733825131623, 0.9747758246779398, 0.9947573922278411, 0.9974113436991716, 0.9995010661399597, 0.977009745440678, 0.022592485854928374, 0.9890989949118526, 0.9989431290044795, 0.9933462882016195, 0.9964010452327311, 0.998381364070753, 0.9955968804988237, 0.9985524984392491, 0.9929231368115514, 0.9982428778602408, 0.0016321158845047877, 0.999078935086509, 0.3970067156795722, 0.602996485872173, 0.9998403376649397, 0.9992295441225396, 0.9999114873367693, 0.9987203338625711, 0.9983417839673535, 0.9949252404447709, 0.9989069188044715, 0.9970670669845573, 0.998366311976505, 0.9968775878519731, 0.9965500976472734, 0.9972339738603866, 0.9954531468542521, 0.998797099800631, 0.9955963013710682, 0.9967120885080349, 0.9958373421535005, 0.9867618541657854, 0.9991464747598086, 0.9977301091041302, 0.9978012258493514, 0.9986931890085606, 0.9995002254445521, 0.9971492290919642, 0.998175373823328, 0.9977003373637197, 0.9982804506798847, 0.9978638749306442, 0.9997945703815472, 0.9961355877792792, 0.9953513475983834, 0.9981775152959211, 0.999518629214706, 0.9998167498388265, 0.9975583035185994, 0.9966945214394058, 0.9907939818510366, 0.9987556197931181, 0.9943107727085103, 0.994260043435401, 0.9997380775424667, 0.9946864512730181, 0.9998011447714558, 0.46288862027149263, 0.5370304994133017, 0.9933897084568704, 0.9938367878615522, 0.9999312173303729, 0.9989716856495893, 0.9954816179986763, 0.0320715137444581, 0.9675314680770872, 0.9993009344511496, 0.9997173154809198, 0.9959200234692496, 0.9995434812040367, 0.9869941107392407, 0.9995751963049911, 0.985182304396143, 0.9957582283231509, 0.9967909174908225, 0.9940522795151212, 0.9942222602264367, 0.9933592500998086, 0.1118505924603858, 0.8879367208477996, 0.9945189945708621, 0.9978187594705547, 0.003845572117811863, 0.99523406408971, 0.9963183219235392, 0.9997237419296766, 0.9973987672104045, 0.9995115206277165, 0.995379107041306, 0.9997610566788124, 0.9941765242663307, 0.9903542711098368, 0.9995952207600162, 0.9995564481753977, 0.9996207557008685, 0.9996218925158559, 0.9995278615727755, 0.9997563099368376, 0.9991941307766812, 0.9962862319748035, 0.9972208608147592, 0.9970373593899124, 0.9948684347083014, 0.9982354609178945, 0.996298969765847, 0.9969309047298016, 0.9900770802102039, 0.9903307394006731, 0.9995176188083932, 0.9975729404198546, 0.983843419644281, 0.01581883122250742, 0.9964877427130302, 0.9982401431159191, 0.9996980728467388, 0.9961555439611187, 0.9990483655553004, 0.9969276769627261, 0.9991432714043733, 0.9993946616535739, 0.9993334238073953, 0.9994025887711828, 0.994958299762211, 0.9804777966544902, 0.9997884838554268, 0.9948778895930702, 0.9975045550476569, 0.9928214074279792, 0.998325891213546, 0.9998041817026596, 0.9903497053092403, 0.9957396354372848, 0.999502396857175, 0.9932690843867484, 0.9963397457516936, 0.9955214773750909, 0.020450260714536376, 0.979340263107242, 0.9985639876377707, 0.9985420852946073, 0.9872906108243721, 0.9930295107921429, 0.9984546671302694, 0.9996747323917097, 0.9995468513808895, 0.9933127652814512, 0.9961302189039178, 0.9954295042854843, 0.9999078555507527, 0.9913385341147192, 0.9970307727838889, 0.9996801700323652, 0.9994656477336127, 0.991330324071248, 0.9963528587547583, 0.9976873905347902, 0.9981051014356888, 0.9999017599629971, 0.9899662219729224, 0.9970956572087403, 0.9911405733112436, 0.999780822056938, 0.9970725550727033, 0.9971474748986755, 0.9969043241806529, 0.9956234904805079, 0.996726237910557, 0.9997742773538815, 0.9920756282708975, 0.9994652370904141, 0.9982034811258027, 0.9995439694995495, 0.9967442670659121, 0.9935659561341178, 0.9947693849498004, 0.9998070455912381, 0.9960479593884558, 0.9968212773413344, 0.9993206628969055, 0.9992014590872023, 0.9986635650890039, 0.998449642080855, 0.9996067552614293, 0.15700031565216505, 0.8429460246561603, 0.9963540426946328, 0.994861678802099, 0.9993504027533812, 0.9987717587307876, 0.001090229936758895, 0.9955739194051093, 0.9975329253367204, 0.9992534746034767, 0.9902535882456901, 0.9998571663299665, 0.9953874531977046, 0.9935216675043317, 0.9958280165676017, 0.9971394876526147, 0.9993288094745311, 0.9983587746043449, 0.9971599605133159, 0.9994530263378054, 0.9970148277809925, 0.9967325794961324, 0.998822948746094, 0.9993919886885113, 0.9986389296907271, 0.999390649314713, 0.9460780333507335, 0.05385075766937874, 0.9954820840784461, 0.9960728206741172, 0.9877231627919464, 0.0015603809540350195, 0.9981979874526883, 0.9993653769650336, 0.9911193208274967, 0.9964372549671993, 0.9757439078396394, 0.02394860669324488, 0.99327066491803, 0.9957316032255586, 0.988057468972853, 0.9981671992686634], \"Term\": [\"actually\", \"add\", \"addition\", \"adjustable\", \"adorable\", \"agree\", \"airy\", \"also\", \"amount\", \"ankle\", \"appear\", \"appreciate\", \"arrive\", \"available\", \"avoid\", \"back\", \"base\", \"bead\", \"beading\", \"beautiful\", \"beautiful\", \"beauty\", \"become\", \"beige\", \"bell\", \"big\", \"bit\", \"blue\", \"bodice\", \"boot\", \"bootie\", \"boxy\", \"bring\", \"broad_shoulder\", \"bulky\", \"busty\", \"buy\", \"buy\", \"buying\", \"cardigan\", \"cashmere\", \"casual\", \"certainly\", \"change\", \"chest\", \"chic\", \"cinch\", \"classy\", \"clingy\", \"close\", \"closet\", \"clothe\", \"clothing\", \"coat\", \"collar\", \"color\", \"come\", \"comfortable\", \"comfy\", \"complaint\", \"compliment\", \"consider\", \"construction\", \"cool\", \"cotton\", \"couple\", \"coverage\", \"cozy\", \"cream\", \"create\", \"cuff\", \"curve\", \"cut\", \"cute\", \"dd\", \"depend\", \"detail\", \"difficult\", \"disappoint\", \"disappointing\", \"dress\", \"dressy\", \"due\", \"easily\", \"edge\", \"embroidery\", \"even\", \"event\", \"ever\", \"excellent\", \"exception\", \"exchange\", \"expect\", \"expectation\", \"extremely\", \"eye\", \"fabric\", \"fairly\", \"fall\", \"favorite\", \"feminine\", \"find\", \"first\", \"fit\", \"fit\", \"flair\", \"flare\", \"flatter\", \"flattering\", \"flattering\", \"flimsy\", \"flow\", \"forgive\", \"form_fitte\", \"frame\", \"frumpy\", \"full\", \"garment\", \"get\", \"get\", \"glad\", \"go\", \"go\", \"good\", \"gorgeous\", \"great\", \"green\", \"grey\", \"half\", \"happy\", \"hard\", \"heavy\", \"heel\", \"hemline\", \"hide\", \"highly_recommend\", \"hip\", \"hole\", \"hour\", \"hug\", \"image\", \"inch\", \"interesting\", \"issue\", \"jacket\", \"jean\", \"jumpsuit\", \"knee\", \"knit\", \"know\", \"lace\", \"large\", \"last\", \"leather\", \"leave\", \"legging\", \"length\", \"less\", \"let\", \"lie\", \"like\", \"linen\", \"lining\", \"little\", \"live\", \"long\", \"look\", \"look\", \"looking\", \"lounge\", \"love\", \"low\", \"mail\", \"make\", \"make\", \"many\", \"material\", \"mean\", \"medium\", \"mid\", \"model\", \"modest\", \"money\", \"month\", \"motif\", \"move\", \"movement\", \"much\", \"much\", \"name\", \"neckline\", \"need\", \"need\", \"new\", \"nice\", \"odd\", \"online\", \"opt\", \"order\", \"outfit\", \"overly\", \"pair\", \"pant\", \"perfect\", \"perfectly\", \"person\", \"petite\", \"piece\", \"pilcro\", \"place\", \"point\", \"possible\", \"pound\", \"previous\", \"product\", \"proportion\", \"provide\", \"purchase\", \"put\", \"quality\", \"quality\", \"rather\", \"read\", \"really\", \"reason\", \"receive\", \"recently\", \"regular\", \"retailer\", \"return\", \"review\", \"rich\", \"ridiculous\", \"right\", \"rise\", \"roll\", \"romantic\", \"roomy\", \"run\", \"sack\", \"sadly\", \"say\", \"scratchy\", \"seam\", \"second\", \"see\", \"see\", \"send\", \"several\", \"sew\", \"shapeless\", \"sheer\", \"shirt\", \"short\", \"shrink\", \"similar\", \"simple\", \"size\", \"sizing\", \"skin\", \"skirt\", \"sleeve\", \"sleeveless\", \"slim\", \"slimme\", \"slit\", \"small\", \"snag\", \"snug\", \"sock\", \"soft\", \"sometimes\", \"special\", \"staple\", \"state\", \"stiff\", \"still\", \"stitch\", \"store\", \"stretchy\", \"style\", \"substantial\", \"subtle\", \"suit\", \"sweater\", \"sweatshirt\", \"tailor\", \"take\", \"tank\", \"texture\", \"thick\", \"thin\", \"think\", \"think\", \"thread\", \"throw\", \"time\", \"top\", \"top\", \"totally\", \"touch\", \"true\", \"truly\", \"try\", \"tunic\", \"turn\", \"underneath\", \"unflattering\", \"usually\", \"versatile\", \"version\", \"waist\", \"waiste\", \"walk\", \"wardrobe\", \"warm\", \"wash\", \"way\", \"wear\", \"wear\", \"weather\", \"wedding\", \"week\", \"well\", \"well\", \"wide\", \"width\", \"wool\", \"work\", \"work\", \"worry\", \"wrinkle\", \"young\", \"zipper\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [5, 10, 8, 3, 1, 4, 2, 6, 7, 9]};\n",
              "\n",
              "function LDAvis_load_lib(url, callback){\n",
              "  var s = document.createElement('script');\n",
              "  s.src = url;\n",
              "  s.async = true;\n",
              "  s.onreadystatechange = s.onload = callback;\n",
              "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
              "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "}\n",
              "\n",
              "if(typeof(LDAvis) !== \"undefined\"){\n",
              "   // already loaded: just create the visualization\n",
              "   !function(LDAvis){\n",
              "       new LDAvis(\"#\" + \"ldavis_el5881367380854545609597299477\", ldavis_el5881367380854545609597299477_data);\n",
              "   }(LDAvis);\n",
              "}else if(typeof define === \"function\" && define.amd){\n",
              "   // require.js is available: use it to load d3/LDAvis\n",
              "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
              "   require([\"d3\"], function(d3){\n",
              "      window.d3 = d3;\n",
              "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "        new LDAvis(\"#\" + \"ldavis_el5881367380854545609597299477\", ldavis_el5881367380854545609597299477_data);\n",
              "      });\n",
              "    });\n",
              "}else{\n",
              "    // require.js not available: dynamically load d3 & LDAvis\n",
              "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
              "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "                 new LDAvis(\"#\" + \"ldavis_el5881367380854545609597299477\", ldavis_el5881367380854545609597299477_data);\n",
              "            })\n",
              "         });\n",
              "}\n",
              "</script>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvtNAgXwJMFD"
      },
      "source": [
        "## 15. How to interpret pyLDAvis’s output?\n",
        "\n",
        "Each bubble on the left-hand side plot represents a topic. The larger the bubble, the more prevalent is that topic.\n",
        "\n",
        "A good topic model will have fairly big, non-overlapping bubbles scattered throughout the chart instead of being clustered in one quadrant.\n",
        "\n",
        "A model with too many topics, will typically have many overlaps, small sized bubbles clustered in one region of the chart.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efQl15CYJMFB"
      },
      "source": [
        "## 16. Compute Model Perplexity and Coherence Score\n",
        "\n",
        "Model perplexity and [topic coherence](https://rare-technologies.com/what-is-topic-coherence/) provide a convenient measure to judge how good a given topic model is. In my experience, topic coherence score, in particular, has been more helpful."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuud9M0VJMFC",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05a70062-8d0b-459c-b2fa-779f3cfbd65c"
      },
      "source": [
        "# Compute Perplexity\n",
        "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
        "\n",
        "# Compute Coherence Score\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Perplexity:  -6.841313739927716\n",
            "\n",
            "Coherence Score:  0.3057667247711376\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqEb9HL-JMFI"
      },
      "source": [
        "Those were the topics for the chosen LDA model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7-8i9EO6qIQ"
      },
      "source": [
        "### All Rights Reserved.\n",
        "\n",
        "This notebook is proprietary content of machinelearningplus.com. This can be shared solely for educational purposes, with due credits to machinelearningplus.com\n",
        "\n",
        "The original notebook was published by **Selva Prabhakaran** [online](https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/).\n",
        "\n",
        "There is an additional segment in Prabhakan's notebook that builds an LDA Mallet model. It has fallen prey to obsolescence &mdash; I really should take the time to fix it."
      ]
    }
  ]
}